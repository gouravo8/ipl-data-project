{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e901c16-a894-42ff-8d41-26a939407b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to ipl_2025_top_buys.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to ESPN Cricinfo IPL 2025 Top Buys Auction page\n",
    "driver.get(\"https://www.espncricinfo.com/auction/ipl-2025-auction-1460972/top-buys\")\n",
    "\n",
    "# Wait for the table to load\n",
    "try:\n",
    "    WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'ds-w-full')]\")))\n",
    "    time.sleep(5)  # Extra time to ensure full loading\n",
    "except Exception as e:\n",
    "    print(\"Timeout Error: Could not find the expected elements.\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Locate auction player table\n",
    "data = []\n",
    "rows = soup.select(\"table tbody tr\")  # Selecting rows in the table\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        columns = row.find_all(\"td\")  # Extract columns for each player\n",
    "        if len(columns) >= 5:  # Ensure the row contains all expected columns\n",
    "            name = columns[0].get_text(strip=True)  # Player Name\n",
    "            team = columns[1].get_text(strip=True)  # Team Abbreviation\n",
    "            role = columns[2].get_text(strip=True)  # Role (BAT, BOWL, AR)\n",
    "            base_price = columns[3].get_text(strip=True)  # Base Price\n",
    "            final_price = columns[4].get_text(strip=True)  # Final Price\n",
    "            \n",
    "            # Append data to the list\n",
    "            data.append([name, team, role, base_price, final_price])\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Convert data to DataFrame and save to CSV\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"Player Name\", \"Team\", \"Role\", \"Base Price\", \"Final Price\"])\n",
    "    df.to_csv(\"ipl_2025_top_buys.csv\", index=False)\n",
    "    print(\"Scraping completed! Data saved to ipl_2025_top_buys.csv\")\n",
    "else:\n",
    "    print(\"No auction data found. The page structure might have changed.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae771c3-3d35-4cc3-bc44-89db63edf897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to 'ipl_2025_sold_players.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Runs in headless mode for efficiency\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to the Sold Players page\n",
    "url = \"https://www.espncricinfo.com/auction/ipl-2025-auction-1460972/sold-players\"\n",
    "driver.get(url)\n",
    "\n",
    "# Scroll to load all rows dynamically\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for rows to load\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Wait for the table to load after scrolling\n",
    "try:\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'ds-w-full')]\"))\n",
    "    )\n",
    "    time.sleep(5)  # Additional wait to ensure all content is loaded\n",
    "except Exception as e:\n",
    "    print(\"Timeout Error: Could not locate the table.\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Locate the player table\n",
    "data = []\n",
    "rows = soup.select(\"table tbody tr\")  # Extract rows from the player table\n",
    "\n",
    "# Extract data from rows\n",
    "for row in rows:\n",
    "    try:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 5:  # Ensure all necessary columns are present\n",
    "            player_name = columns[0].get_text(strip=True)  # Player Name\n",
    "            team = columns[1].get_text(strip=True)  # Team\n",
    "            role = columns[2].get_text(strip=True)  # Role\n",
    "            base_price = columns[3].get_text(strip=True)  # Base Price\n",
    "            final_price = columns[4].get_text(strip=True)  # Final Price\n",
    "\n",
    "            # Append player data to the list\n",
    "            data.append([player_name, team, role, base_price, final_price])\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"Player Name\", \"Team\", \"Role\", \"Base Price\", \"Final Price\"])\n",
    "    df.to_csv(\"ipl_2025_sold_players.csv\", index=False)\n",
    "    print(\"Scraping completed! Data saved to 'ipl_2025_sold_players.csv'.\")\n",
    "else:\n",
    "    print(\"No data found. Please check the selectors or webpage structure.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31b2102-eca3-40d4-8cc2-9f783643bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to ipl_2025_unsold_players.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to the ESPN Cricinfo IPL 2025 Unsold Players page\n",
    "driver.get(\"https://www.espncricinfo.com/auction/ipl-2025-auction-1460972/unsold-players\")\n",
    "\n",
    "# Wait for the table to load\n",
    "try:\n",
    "    WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'ds-w-full')]\")))\n",
    "    time.sleep(5)  # Extra time to ensure full loading\n",
    "except Exception as e:\n",
    "    print(\"Timeout Error: Could not find the expected elements.\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Locate auction player table\n",
    "data = []\n",
    "rows = soup.select(\"table tbody tr\")  # Selecting rows in the table\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        columns = row.find_all(\"td\")  # Extract columns for each player\n",
    "        if len(columns) >= 3:  # Adjust based on Unsold table structure\n",
    "            name = columns[0].get_text(strip=True)  # Player Name\n",
    "            role = columns[1].get_text(strip=True)  # Role\n",
    "            base_price = columns[2].get_text(strip=True)  # Base Price\n",
    "            \n",
    "            # Append data to the list\n",
    "            data.append([name, role, base_price])\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Convert data to DataFrame and save to CSV\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"Player Name\", \"Role\", \"Base Price\"])\n",
    "    df.to_csv(\"ipl_2025_unsold_players.csv\", index=False)\n",
    "    print(\"Scraping completed! Data saved to ipl_2025_unsold_players.csv\")\n",
    "else:\n",
    "    print(\"No auction data found. The page structure might have changed.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a72ba0-de67-4c77-a4a2-c1a23fcc8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to ipl_2025_squads.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to IPL 2025 auction squads page\n",
    "url = \"https://www.espncricinfo.com/auction/ipl-2025-auction-1460972\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the squads section to load\n",
    "try:\n",
    "    WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'ds-w-full')]\")))\n",
    "    time.sleep(5)  # Extra loading time\n",
    "except Exception as e:\n",
    "    print(\"Timeout Error: Could not find the expected elements.\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Locate squad tables\n",
    "data = []\n",
    "tables = soup.select(\"table.ds-w-full\")  # Adjusted selector to target squad tables\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        # Extract team name\n",
    "        team_name = table.find_previous(\"a\").get_text(strip=True)  # Get team name from the previous anchor tag\n",
    "\n",
    "        # Extract purse details\n",
    "        purse_info = table.find_previous(\"div\").get_text(strip=True)  # Get purse spent, purse left, etc.\n",
    "        purse_details = purse_info.split(\"|\")  # Splitting details for structured data\n",
    "\n",
    "        if len(purse_details) >= 4:\n",
    "            purse_spent = purse_details[0]\n",
    "            purse_left = purse_details[1]\n",
    "            total_players = purse_details[2]\n",
    "            overseas_players = purse_details[3]\n",
    "        else:\n",
    "            purse_spent = purse_left = total_players = overseas_players = \"N/A\"\n",
    "\n",
    "        # Extract player details\n",
    "        rows = table.select(\"tbody tr\")  # Selecting player rows\n",
    "        for row in rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            if len(columns) >= 4:\n",
    "                player_name = columns[0].get_text(strip=True)\n",
    "                role = columns[1].get_text(strip=True)\n",
    "                base_price = columns[2].get_text(strip=True)\n",
    "                sold_price = columns[3].get_text(strip=True)\n",
    "\n",
    "                data.append([team_name, purse_spent, purse_left, total_players, overseas_players,\n",
    "                             player_name, role, base_price, sold_price])\n",
    "\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Convert data to DataFrame and save to CSV\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"Team\", \"Purse Spent\", \"Purse Left\", \"Total Players\", \"Overseas Players\",\n",
    "                                     \"Player Name\", \"Role\", \"Base Price\", \"Sold Price\"])\n",
    "    df.to_csv(\"ipl_2025_squads.csv\", index=False)\n",
    "    print(\"Scraping completed! Data saved to ipl_2025_squads.csv\")\n",
    "else:\n",
    "    print(\"No squad data found. The page structure might have changed.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee1856d-f13e-496b-8966-3e353eef9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to 'ipl_2025_all_players.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run without opening a browser\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to the \"All Players\" page\n",
    "url = \"https://www.espncricinfo.com/auction/ipl-2025-auction-1460972/all-players\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the table to load initially\n",
    "try:\n",
    "    WebDriverWait(driver, 40).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'ds-w-full')]\"))\n",
    "    )\n",
    "    time.sleep(5)\n",
    "except Exception as e:\n",
    "    print(\"Timeout Error: Could not locate the table.\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Scroll to load all rows dynamically\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for rows to load\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse the page with BeautifulSoup after scrolling\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "rows = soup.select(\"table tbody tr\")  # Extract all rows from the table\n",
    "\n",
    "# Extract player data from rows\n",
    "data = []\n",
    "for row in rows:\n",
    "    try:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 5:\n",
    "            player_name = columns[0].get_text(strip=True)\n",
    "            role = columns[1].get_text(strip=True)\n",
    "            base_price = columns[2].get_text(strip=True)\n",
    "            sold_price = columns[3].get_text(strip=True)\n",
    "            team = columns[4].get_text(strip=True)\n",
    "\n",
    "            # Add player data to the list\n",
    "            data.append([player_name, role, base_price, sold_price, team])\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"Player Name\", \"Role\", \"Base Price\", \"Sold Price\", \"Team\"])\n",
    "    df.to_csv(\"ipl_2025_all_players.csv\", index=False)\n",
    "    print(\"Scraping completed! Data saved to 'ipl_2025_all_players.csv'.\")\n",
    "else:\n",
    "    print(\"No data found. Please check the selectors or webpage structure.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38f6ee5-eda3-44a1-9a50-c3907669998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data already in tabular format. Saved to ipl_2025_all_players_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = \"ipl_2025_all_players.csv\"\n",
    "output_file = \"ipl_2025_all_players_cleaned.csv\"\n",
    "\n",
    "# Read the CSV safely, skipping any badly formatted rows\n",
    "try:\n",
    "    df = pd.read_csv(input_file, header=None, on_bad_lines='skip')\n",
    "\n",
    "    # If it's in one column, split it\n",
    "    if df.shape[1] == 1:\n",
    "        print(\"Splitting single-column data...\")\n",
    "        df_split = df[0].str.split(',', expand=True)\n",
    "\n",
    "        # Check for expected columns (e.g., name, base price, sold price, team)\n",
    "        if df_split.shape[1] >= 4:\n",
    "            df_split.columns = [\"Player Name\", \"Base Price\", \"Sold Price\", \"Team\"]\n",
    "        else:\n",
    "            df_split.columns = [f\"Column {i+1}\" for i in range(df_split.shape[1])]\n",
    "\n",
    "        df_split.to_csv(output_file, index=False)\n",
    "        print(\"✅ Data cleaned and saved to\", output_file)\n",
    "    else:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(\"✅ Data already in tabular format. Saved to\", output_file)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error processing file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "708da988-01cb-4049-9896-937ef63bfa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: []\n",
      "✅ Data saved to ipl_2025_most_runs.csv\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "url = \"https://www.espncricinfo.com/records/tournament/batting-most-runs-career/indian-premier-league-2025-16622\"\n",
    "\n",
    "driver = uc.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# Try to extract headers\n",
    "th_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
    "headers = [th.text.strip() for th in th_elements]\n",
    "print(\"Headers:\", headers)\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "    cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Fallback: use first row as header if <th> tags were empty\n",
    "if not headers and rows:\n",
    "    headers = rows[0]\n",
    "    rows = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"ipl_2025_most_runs.csv\", index=False)\n",
    "print(\"✅ Data saved to ipl_2025_most_runs.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf8733d-ba73-45fb-93d9-9ad0d6b53b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: []\n",
      "✅ Data saved to ipl_2025_most_wickets.csv\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Updated URL for most wickets\n",
    "url = \"https://www.espncricinfo.com/records/tournament/bowling-most-wickets-career/indian-premier-league-2025-16622\"\n",
    "\n",
    "driver = uc.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# Try to extract headers\n",
    "th_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
    "headers = [th.text.strip() for th in th_elements]\n",
    "print(\"Headers:\", headers)\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "    cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Fallback: use first row as header if <th> tags were empty\n",
    "if not headers and rows:\n",
    "    headers = rows[0]\n",
    "    rows = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"ipl_2025_most_wickets.csv\", index=False)\n",
    "print(\"✅ Data saved to ipl_2025_most_wickets.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd8d3fbb-2ea4-4b99-ac8e-d8e7c0633a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: []\n",
      "✅ Data saved to ipl_2024_most_runs.csv\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Updated URL for IPL 2024 most runs\n",
    "url = \"https://www.espncricinfo.com/records/tournament/batting-most-runs-career/indian-premier-league-2024-15940\"\n",
    "\n",
    "driver = uc.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# Try to extract headers\n",
    "th_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
    "headers = [th.text.strip() for th in th_elements]\n",
    "print(\"Headers:\", headers)\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "    cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Fallback: use first row as header if <th> tags were empty\n",
    "if not headers and rows:\n",
    "    headers = rows[0]\n",
    "    rows = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"ipl_2024_most_runs.csv\", index=False)\n",
    "print(\"✅ Data saved to ipl_2024_most_runs.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bb3c1b1-dd88-421f-a828-0919ec968e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: []\n",
      "✅ Data saved to ipl_2024_most_wickets.csv\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL for IPL 2024 most wickets\n",
    "url = \"https://www.espncricinfo.com/records/tournament/bowling-most-wickets-career/indian-premier-league-2024-15940\"\n",
    "\n",
    "driver = uc.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# Extract headers\n",
    "th_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
    "headers = [th.text.strip() for th in th_elements]\n",
    "print(\"Headers:\", headers)\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "    cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Fallback: use first row as header if <th> tags were empty\n",
    "if not headers and rows:\n",
    "    headers = rows[0]\n",
    "    rows = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"ipl_2024_most_wickets.csv\", index=False)\n",
    "print(\"✅ Data saved to ipl_2024_most_wickets.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a74b315e-5dd2-4d62-806e-178a65d2b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['Player', 'Team', 'Total Impact', 'Impact/Mat', 'Matches', 'Runs', 'Wkts']\n",
      "✅ Data saved to ipl_2024_mvp.csv\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL for IPL 2024 Most Valuable Players\n",
    "url = \"https://www.espncricinfo.com/series/indian-premier-league-2024-1410320/most-valuable-players\"\n",
    "\n",
    "driver = uc.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "# Extract headers\n",
    "th_elements = table.find_elements(By.TAG_NAME, \"th\")\n",
    "headers = [th.text.strip() for th in th_elements]\n",
    "print(\"Headers:\", headers)\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "    cells = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "    if cells:\n",
    "        rows.append(cells)\n",
    "\n",
    "# Fallback: use first row as header if <th> tags were empty\n",
    "if not headers and rows:\n",
    "    headers = rows[0]\n",
    "    rows = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"ipl_2024_mvp.csv\", index=False)\n",
    "print(\"✅ Data saved to ipl_2024_mvp.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce95fef-a632-435b-9364-2e1b11e77132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
